{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"authorship_tag":"ABX9TyNxGX8cMKxjc1zuAklq21d6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"af7d2a4837a94b10b0b9cf7a84d5ce94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f4a1b98859049e4b05c149022a8e0c2","IPY_MODEL_8b7a2620bfc748a19c08b26d435c4736","IPY_MODEL_6e08a7d2e245471793e1bf2e889ac9d4"],"layout":"IPY_MODEL_082360bb01c2429bb81898439d430ec6"}},"5f4a1b98859049e4b05c149022a8e0c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6c2ebc8d67f4b27958764f51e4d2982","placeholder":"​","style":"IPY_MODEL_182783dec8b14f60956885e0f76af515","value":"100%"}},"8b7a2620bfc748a19c08b26d435c4736":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b923af5a9a94ce598d1be0e9dfa6441","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_651931f59f5044ef93219ef4cbb2fca1","value":200}},"6e08a7d2e245471793e1bf2e889ac9d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_060bd5ddef5e4d1da5b519404f464995","placeholder":"​","style":"IPY_MODEL_03bbeeacdf64408498cfcfec5b009577","value":" 200/200 [1:00:50&lt;00:00, 18.00s/it]"}},"082360bb01c2429bb81898439d430ec6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6c2ebc8d67f4b27958764f51e4d2982":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"182783dec8b14f60956885e0f76af515":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b923af5a9a94ce598d1be0e9dfa6441":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"651931f59f5044ef93219ef4cbb2fca1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"060bd5ddef5e4d1da5b519404f464995":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03bbeeacdf64408498cfcfec5b009577":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import os\n","import math\n","import matplotlib.pyplot as plt\n","from datetime import datetime, timedelta\n","import statsmodels.api as sm\n","from statsmodels.tsa.stattools import adfuller\n","pd.set_option('display.max_columns',100)\n","from google.colab import drive\n","from scipy.stats import pearsonr\n","import torch\n","from torch.nn.modules.module import Module\n","from torch.autograd import Variable\n","from torch.nn.parameter import Parameter\n","import torch.utils as utils\n","from torch import nn\n","import torch.nn.functional as F\n","from tqdm.notebook import tqdm_notebook\n","from sklearn.preprocessing import MinMaxScaler,StandardScaler\n","import torch.optim as optim\n","drive.mount('/content/drive')\n","root_dir = \"/content/drive/MyDrive/Colab Notebooks/Public Folder: SafeGraph Group /Safegraph Data/Summary\"\n","os.chdir(f\"{root_dir}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7fp1B3De3cHf","executionInfo":{"status":"ok","timestamp":1661630054184,"user_tz":240,"elapsed":26681,"user":{"displayName":"Bowen Han","userId":"18105580727989418474"}},"outputId":"96fecdf9-2339-4b53-df20-d045c633ae79"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"yQtijreV3Xzn","executionInfo":{"status":"ok","timestamp":1661630055114,"user_tz":240,"elapsed":936,"user":{"displayName":"Bowen Han","userId":"18105580727989418474"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cuda = True if torch.cuda.is_available() else False\n","drop_rate = 0.5\n","time_step = 5\n","epochs = 200\n","feature_matrix = np.load('feature_matrix.npy')"]},{"cell_type":"code","source":["scaler = StandardScaler()\n","def normalization(feature_matrix):\n","  feature_mat = np.zeros((feature_matrix.shape[0],feature_matrix.shape[1],feature_matrix.shape[2]))\n","  target = np.zeros((feature_matrix.shape[0],feature_matrix.shape[1]))\n","  for i in range(feature_matrix.shape[0]):\n","    feature_vector=feature_matrix[i]\n","    f1 = scaler.fit_transform(feature_vector[:,0].reshape(-1,1)).reshape(-1,1)\n","    f2 = scaler.fit_transform(feature_vector[:,1].reshape(-1,1)).reshape(-1,1)\n","    visit_nums = scaler.fit_transform(feature_vector[:,2].reshape(-1,1)).reshape(-1,1)\n","    feature_vector = np.stack([f1,f2,visit_nums],axis=1).reshape(feature_matrix.shape[1],feature_matrix.shape[2])\n","    feature_mat[i] = feature_vector\n","    target[i] = visit_nums.flatten()\n","  return feature_mat, target\n","features = feature_matrix[:,:,[0,3,4]]\n","features, results = normalization(features)\n","train_idx = int(0.7 * features.shape[0])\n","train_features = features[:train_idx,:,:]\n","test_features = features[train_idx:,:,:]"],"metadata":{"id":"eb4eN4ji3bi6","executionInfo":{"status":"ok","timestamp":1661630301430,"user_tz":240,"elapsed":160,"user":{"displayName":"Bowen Han","userId":"18105580727989418474"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def get_features(features, index):\n","  f = features[:,index,:]\n","  new = np.zeros((len(f) - time_step, time_step, 3))\n","  target = np.zeros((len(f) - time_step, 1))\n","  for i in range(len(f) - time_step):\n","    new[i] = f[i:i+time_step]\n","    target[i] = f[i+time_step, 2]\n","  return new, target"],"metadata":{"id":"hU_bEoMi3gTR","executionInfo":{"status":"ok","timestamp":1661630303682,"user_tz":240,"elapsed":166,"user":{"displayName":"Bowen Han","userId":"18105580727989418474"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class MLP(torch.nn.Module):\n","    def __init__(self, n_i, n_h, n_o):\n","        super(MLP, self).__init__()\n","        self.linear1 = nn.Linear(n_i, n_h)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(n_h, n_o)\n","    def forward(self, input):\n","        input = self.linear1(input)\n","        input = self.relu(input)\n","        input = F.dropout(input, p=drop_rate, training=self.training)\n","        input = self.linear2(input)\n","        return input\n","###Convert the data to tensor \n","def totensor(data):\n","    return torch.Tensor(data).to(device)\n","\n","class LSTM(nn.Module):\n","    def __init__(self):\n","        super(LSTM,self).__init__()\n","        self.mlp = MLP(time_step,32,1)#input is the num of time_step\n","        self.lstm1 = nn.LSTM(input_size=3,hidden_size=128,num_layers=2,batch_first=True)#3 is for number of features\n","        self.lstm2 = nn.LSTM(input_size=128,hidden_size=1,num_layers=2,batch_first=True)\n","    def forward(self, x):\n","        x1,x2,x3 = x.size()\n","        h1, (h1_T,c1_T) = self.lstm1(x)\n","        h2, (h2_T, c2_T) = self.lstm2(h1)\n","        h2 = h2.view(x1,-1)\n","        output= self.mlp(h2)\n","        return output "],"metadata":{"id":"NvFPHxDp3gWC","executionInfo":{"status":"ok","timestamp":1661630304847,"user_tz":240,"elapsed":203,"user":{"displayName":"Bowen Han","userId":"18105580727989418474"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["min_val_loss = np.inf\n","mean_losses = []\n","losses = pd.DataFrame(columns = ['Training Loss', 'Testing Loss'])\n","model = LSTM()\n","loss = nn.MSELoss()\n","optimizer = optim.AdamW(model.parameters(),lr=0.001)\n","for epoch in tqdm_notebook(range(1, epochs + 1)):\n","    mean_loss, n = 0.0 , train_features.shape[1]\n","    train_mean_loss, test_mean_loss = 0, 0\n","    for i in range(n):\n","      train, train_target = get_features(totensor(train_features), i)\n","      train, train_target = totensor(train), totensor(train_target)\n","      test, test_target = get_features(totensor(test_features), i)\n","      test, test_target = totensor(test), totensor(test_target)\n","      y_train, y_test = model(train), model(test)\n","      train_loss, test_loss = loss(y_train, train_target), loss(y_test, test_target)\n","      optimizer.zero_grad()\n","      train_loss.backward()\n","      optimizer.step()\n","      losses.loc[i,'Training Loss'], losses.loc[i,'Testing Loss'] = train_loss.detach().numpy(), test_loss.detach().numpy()\n","    train_loss, test_loss = losses['Training Loss'].mean(), losses['Testing Loss'].mean()\n","    mean_losses.append((train_loss, test_loss))\n","    if train_loss < min_val_loss:\n","      min_val_loss = train_loss\n","    print('Epoch: {:03d} | Lr: {:.20f} |Train loss: {:.8f}|Test loss: {:.8f}'.\\\n","          format(epoch, optimizer.param_groups[0]['lr'], min_val_loss, test_loss))\n","\n","print('\\nTraining finished.\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["af7d2a4837a94b10b0b9cf7a84d5ce94","5f4a1b98859049e4b05c149022a8e0c2","8b7a2620bfc748a19c08b26d435c4736","6e08a7d2e245471793e1bf2e889ac9d4","082360bb01c2429bb81898439d430ec6","b6c2ebc8d67f4b27958764f51e4d2982","182783dec8b14f60956885e0f76af515","7b923af5a9a94ce598d1be0e9dfa6441","651931f59f5044ef93219ef4cbb2fca1","060bd5ddef5e4d1da5b519404f464995","03bbeeacdf64408498cfcfec5b009577"]},"id":"8RVxLRtF3v6t","executionInfo":{"status":"ok","timestamp":1661633957352,"user_tz":240,"elapsed":3651141,"user":{"displayName":"Bowen Han","userId":"18105580727989418474"}},"outputId":"9f29d273-8202-44b0-e139-a370e4f77466"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/200 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7d2a4837a94b10b0b9cf7a84d5ce94"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001 | Lr: 0.00100000000000000002 |Train loss: 0.87677940|Test loss: 0.92106085\n","Epoch: 002 | Lr: 0.00100000000000000002 |Train loss: 0.52883968|Test loss: 0.57368730\n","Epoch: 003 | Lr: 0.00100000000000000002 |Train loss: 0.38273744|Test loss: 0.52325443\n","Epoch: 004 | Lr: 0.00100000000000000002 |Train loss: 0.30990746|Test loss: 0.41723964\n","Epoch: 005 | Lr: 0.00100000000000000002 |Train loss: 0.28445313|Test loss: 0.42778846\n","Epoch: 006 | Lr: 0.00100000000000000002 |Train loss: 0.28445313|Test loss: 0.38844083\n","Epoch: 007 | Lr: 0.00100000000000000002 |Train loss: 0.25962535|Test loss: 0.49303278\n","Epoch: 008 | Lr: 0.00100000000000000002 |Train loss: 0.25962535|Test loss: 0.42399413\n","Epoch: 009 | Lr: 0.00100000000000000002 |Train loss: 0.25418689|Test loss: 0.50550355\n","Epoch: 010 | Lr: 0.00100000000000000002 |Train loss: 0.25418689|Test loss: 0.47527446\n","Epoch: 011 | Lr: 0.00100000000000000002 |Train loss: 0.25418689|Test loss: 0.55059960\n","Epoch: 012 | Lr: 0.00100000000000000002 |Train loss: 0.25418689|Test loss: 0.44150488\n","Epoch: 013 | Lr: 0.00100000000000000002 |Train loss: 0.25418689|Test loss: 0.37862731\n","Epoch: 014 | Lr: 0.00100000000000000002 |Train loss: 0.25204314|Test loss: 0.34444188\n","Epoch: 015 | Lr: 0.00100000000000000002 |Train loss: 0.25204314|Test loss: 0.38815700\n","Epoch: 016 | Lr: 0.00100000000000000002 |Train loss: 0.24588748|Test loss: 0.48339613\n","Epoch: 017 | Lr: 0.00100000000000000002 |Train loss: 0.23993067|Test loss: 0.47186894\n","Epoch: 018 | Lr: 0.00100000000000000002 |Train loss: 0.22100689|Test loss: 0.34498663\n","Epoch: 019 | Lr: 0.00100000000000000002 |Train loss: 0.21330781|Test loss: 0.42661464\n","Epoch: 020 | Lr: 0.00100000000000000002 |Train loss: 0.21330781|Test loss: 0.45095437\n","Epoch: 021 | Lr: 0.00100000000000000002 |Train loss: 0.21330781|Test loss: 0.46056838\n","Epoch: 022 | Lr: 0.00100000000000000002 |Train loss: 0.20199813|Test loss: 0.36574974\n","Epoch: 023 | Lr: 0.00100000000000000002 |Train loss: 0.20199813|Test loss: 0.36919091\n","Epoch: 024 | Lr: 0.00100000000000000002 |Train loss: 0.20199813|Test loss: 0.44214146\n","Epoch: 025 | Lr: 0.00100000000000000002 |Train loss: 0.20199813|Test loss: 0.29233120\n","Epoch: 026 | Lr: 0.00100000000000000002 |Train loss: 0.20199813|Test loss: 0.42560008\n","Epoch: 027 | Lr: 0.00100000000000000002 |Train loss: 0.20199813|Test loss: 0.35576443\n","Epoch: 028 | Lr: 0.00100000000000000002 |Train loss: 0.20142788|Test loss: 0.31412940\n","Epoch: 029 | Lr: 0.00100000000000000002 |Train loss: 0.20103263|Test loss: 0.41526720\n","Epoch: 030 | Lr: 0.00100000000000000002 |Train loss: 0.20103263|Test loss: 0.35051148\n","Epoch: 031 | Lr: 0.00100000000000000002 |Train loss: 0.19888273|Test loss: 0.34296475\n","Epoch: 032 | Lr: 0.00100000000000000002 |Train loss: 0.19888273|Test loss: 0.39349402\n","Epoch: 033 | Lr: 0.00100000000000000002 |Train loss: 0.19144617|Test loss: 0.31093839\n","Epoch: 034 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.35945030\n","Epoch: 035 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.34301754\n","Epoch: 036 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.37459705\n","Epoch: 037 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.37642264\n","Epoch: 038 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.37888046\n","Epoch: 039 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.37229489\n","Epoch: 040 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.33493717\n","Epoch: 041 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.39837198\n","Epoch: 042 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.33863566\n","Epoch: 043 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.38157935\n","Epoch: 044 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.33545989\n","Epoch: 045 | Lr: 0.00100000000000000002 |Train loss: 0.17881037|Test loss: 0.32530695\n","Epoch: 046 | Lr: 0.00100000000000000002 |Train loss: 0.17645132|Test loss: 0.40470529\n","Epoch: 047 | Lr: 0.00100000000000000002 |Train loss: 0.17645132|Test loss: 0.49720123\n","Epoch: 048 | Lr: 0.00100000000000000002 |Train loss: 0.17645132|Test loss: 0.39377761\n","Epoch: 049 | Lr: 0.00100000000000000002 |Train loss: 0.17645132|Test loss: 0.32260368\n","Epoch: 050 | Lr: 0.00100000000000000002 |Train loss: 0.17645132|Test loss: 0.41236164\n","Epoch: 051 | Lr: 0.00100000000000000002 |Train loss: 0.17645132|Test loss: 0.45030150\n","Epoch: 052 | Lr: 0.00100000000000000002 |Train loss: 0.17645132|Test loss: 0.34851778\n","Epoch: 053 | Lr: 0.00100000000000000002 |Train loss: 0.17645132|Test loss: 0.34509132\n","Epoch: 054 | Lr: 0.00100000000000000002 |Train loss: 0.16837615|Test loss: 0.33691814\n","Epoch: 055 | Lr: 0.00100000000000000002 |Train loss: 0.16837615|Test loss: 0.42665547\n","Epoch: 056 | Lr: 0.00100000000000000002 |Train loss: 0.16837615|Test loss: 0.39317700\n","Epoch: 057 | Lr: 0.00100000000000000002 |Train loss: 0.16837615|Test loss: 0.34889902\n","Epoch: 058 | Lr: 0.00100000000000000002 |Train loss: 0.15927466|Test loss: 0.38803541\n","Epoch: 059 | Lr: 0.00100000000000000002 |Train loss: 0.15927466|Test loss: 0.39228499\n","Epoch: 060 | Lr: 0.00100000000000000002 |Train loss: 0.15652980|Test loss: 0.40597752\n","Epoch: 061 | Lr: 0.00100000000000000002 |Train loss: 0.15652980|Test loss: 0.34347275\n","Epoch: 062 | Lr: 0.00100000000000000002 |Train loss: 0.15652980|Test loss: 0.37219796\n","Epoch: 063 | Lr: 0.00100000000000000002 |Train loss: 0.15652980|Test loss: 0.33266038\n","Epoch: 064 | Lr: 0.00100000000000000002 |Train loss: 0.15652980|Test loss: 0.38466898\n","Epoch: 065 | Lr: 0.00100000000000000002 |Train loss: 0.15652980|Test loss: 0.36411444\n","Epoch: 066 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36986767\n","Epoch: 067 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36716866\n","Epoch: 068 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.33710741\n","Epoch: 069 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.41833286\n","Epoch: 070 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.34906931\n","Epoch: 071 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.33335864\n","Epoch: 072 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.33721057\n","Epoch: 073 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.34792121\n","Epoch: 074 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36447164\n","Epoch: 075 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.31865895\n","Epoch: 076 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37226158\n","Epoch: 077 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.38504093\n","Epoch: 078 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36750060\n","Epoch: 079 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.30291469\n","Epoch: 080 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36731527\n","Epoch: 081 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36079991\n","Epoch: 082 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.38079616\n","Epoch: 083 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.32183365\n","Epoch: 084 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36519020\n","Epoch: 085 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.32083427\n","Epoch: 086 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.30540885\n","Epoch: 087 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37818599\n","Epoch: 088 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.34778406\n","Epoch: 089 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.43924641\n","Epoch: 090 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.46750905\n","Epoch: 091 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36164619\n","Epoch: 092 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.35367189\n","Epoch: 093 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.40994404\n","Epoch: 094 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37052527\n","Epoch: 095 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.40347985\n","Epoch: 096 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.38724660\n","Epoch: 097 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.32978000\n","Epoch: 098 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37361212\n","Epoch: 099 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.39045847\n","Epoch: 100 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.30694457\n","Epoch: 101 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36136030\n","Epoch: 102 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.41594137\n","Epoch: 103 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37056060\n","Epoch: 104 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37240075\n","Epoch: 105 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.34667856\n","Epoch: 106 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.33387255\n","Epoch: 107 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.38917002\n","Epoch: 108 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36841354\n","Epoch: 109 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.42257056\n","Epoch: 110 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.43387948\n","Epoch: 111 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.43688629\n","Epoch: 112 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.35700190\n","Epoch: 113 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.42083292\n","Epoch: 114 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37046509\n","Epoch: 115 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.36940554\n","Epoch: 116 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.49841773\n","Epoch: 117 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37212165\n","Epoch: 118 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.40752115\n","Epoch: 119 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.35012518\n","Epoch: 120 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.43497698\n","Epoch: 121 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37274113\n","Epoch: 122 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.30855618\n","Epoch: 123 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.40902686\n","Epoch: 124 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.41262914\n","Epoch: 125 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37774622\n","Epoch: 126 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.40028311\n","Epoch: 127 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.41355631\n","Epoch: 128 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.32696012\n","Epoch: 129 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.38243214\n","Epoch: 130 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.34313391\n","Epoch: 131 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.38579979\n","Epoch: 132 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.39093454\n","Epoch: 133 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.39808386\n","Epoch: 134 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.39230820\n","Epoch: 135 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.43471680\n","Epoch: 136 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.39456452\n","Epoch: 137 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.37032822\n","Epoch: 138 | Lr: 0.00100000000000000002 |Train loss: 0.14133164|Test loss: 0.42617337\n","Epoch: 139 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.41640948\n","Epoch: 140 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.45980859\n","Epoch: 141 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.37256494\n","Epoch: 142 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.34617483\n","Epoch: 143 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.38370939\n","Epoch: 144 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.37275205\n","Epoch: 145 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.40585634\n","Epoch: 146 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.42052802\n","Epoch: 147 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.32466732\n","Epoch: 148 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.39117339\n","Epoch: 149 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.40974715\n","Epoch: 150 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.40687139\n","Epoch: 151 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.34336908\n","Epoch: 152 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.39628239\n","Epoch: 153 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.44037071\n","Epoch: 154 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.38836088\n","Epoch: 155 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.43243275\n","Epoch: 156 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.34174231\n","Epoch: 157 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.35800745\n","Epoch: 158 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.37780709\n","Epoch: 159 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.40339836\n","Epoch: 160 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.41624703\n","Epoch: 161 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.39241426\n","Epoch: 162 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.40024252\n","Epoch: 163 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.39309219\n","Epoch: 164 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.39457365\n","Epoch: 165 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.42569204\n","Epoch: 166 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.36955154\n","Epoch: 167 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.36999504\n","Epoch: 168 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.37398718\n","Epoch: 169 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.43358025\n","Epoch: 170 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.29732502\n","Epoch: 171 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.29164908\n","Epoch: 172 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.44987308\n","Epoch: 173 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.35717042\n","Epoch: 174 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.34064574\n","Epoch: 175 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.37278875\n","Epoch: 176 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.35219961\n","Epoch: 177 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.39675200\n","Epoch: 178 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.49291147\n","Epoch: 179 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.44661449\n","Epoch: 180 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.37793842\n","Epoch: 181 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.39366896\n","Epoch: 182 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.42791146\n","Epoch: 183 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.36232898\n","Epoch: 184 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.39466549\n","Epoch: 185 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.40204031\n","Epoch: 186 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.35708840\n","Epoch: 187 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.44293641\n","Epoch: 188 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.34613910\n","Epoch: 189 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.38966533\n","Epoch: 190 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.45428701\n","Epoch: 191 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.43206528\n","Epoch: 192 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.48594599\n","Epoch: 193 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.40162968\n","Epoch: 194 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.47075337\n","Epoch: 195 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.31532154\n","Epoch: 196 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.42037693\n","Epoch: 197 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.42220427\n","Epoch: 198 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.36013677\n","Epoch: 199 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.38339197\n","Epoch: 200 | Lr: 0.00100000000000000002 |Train loss: 0.12671164|Test loss: 0.41804765\n","\n","Training finished.\n","\n"]}]},{"cell_type":"code","source":["mean_losses = np.array(mean_losses)\n","plt.plot(np.arange(200), mean_losses[:,0])\n","plt.plot(np.arange(200), mean_losses[:,1])\n","plt.xlabel('epoch')\n","plt.ylabel('Training Loss')\n","plt.legend(['Train','Test'])"],"metadata":{"id":"HcYKIas53v9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ynYT-1tU3zEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ijxwFvwA3zHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0A66ovG13v_X"},"execution_count":null,"outputs":[]}]}